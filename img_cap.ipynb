{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Final Project: Adding captions to your photos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install__\n",
    "\n",
    "```sh\n",
    "pip install kaggle\n",
    "```\n",
    "\n",
    "__Download__\n",
    "\n",
    "```sh\n",
    "kaggle datasets download -d adityajn105/flickr8k\n",
    "```\n",
    "\n",
    "__Extract__\n",
    "\n",
    "```sh\n",
    "tar -xf flickr8k.zip\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary Python libraries\n",
    "# pip install torch torchvision nltk sklearn pandas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder path where your images are located\n",
    "images_folder = \"./Images/\"\n",
    "\n",
    "# Get all the image file paths in the folder\n",
    "image_paths = [os.path.join(images_folder, filename) for filename in os.listdir(images_folder) if filename.endswith('.jpg')]\n",
    "\n",
    "# Load ResNet50 pre-trained model for feature extraction\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "model = models.resnet50(weights=weights)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove final classification layer\n",
    "model.eval()\n",
    "\n",
    "# Function to extract features from an image for clustering\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model(input_tensor)\n",
    "    return features.flatten().numpy()\n",
    "\n",
    "# Extract features for all images\n",
    "features_list = [extract_features(image_path) for image_path in image_paths]\n",
    "\n",
    "# Perform clustering on the images\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "clusters = kmeans.fit_predict(features_list)\n",
    "\n",
    "# Organize images into clusters\n",
    "cluster_dict = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(clusters):\n",
    "    cluster_dict[cluster_id].append(image_paths[idx])\n",
    "\n",
    "# Split each cluster into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_image_paths = []\n",
    "test_image_paths = []\n",
    "for cluster_id, images in cluster_dict.items():\n",
    "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "    train_image_paths.extend(train_images)\n",
    "    test_image_paths.extend(test_images)\n",
    "\n",
    "# Save the train and test paths to JSON files\n",
    "with open(\"train_image_paths.json\", \"w\") as train_file:\n",
    "    json.dump(train_image_paths, train_file)\n",
    "\n",
    "with open(\"test_image_paths.json\", \"w\") as test_file:\n",
    "    json.dump(test_image_paths, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions from captions.txt or a JSON file\n",
    "captions_dict = {}\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        image_name = parts[0].split(\"#\")[0]\n",
    "        caption = parts[1]\n",
    "        if image_name not in captions_dict:\n",
    "            captions_dict[image_name] = []\n",
    "        captions_dict[image_name].append(caption)\n",
    "\n",
    "# Preprocess captions\n",
    "all_captions = []\n",
    "for captions in captions_dict.values():\n",
    "    all_captions.extend(captions)\n",
    "\n",
    "# Tokenize all captions\n",
    "tokenized_captions = [nltk.word_tokenize(caption.lower()) for caption in all_captions]\n",
    "\n",
    "# Build vocabulary\n",
    "word_counts = Counter()\n",
    "for caption in tokenized_captions:\n",
    "    word_counts.update(caption)\n",
    "\n",
    "vocab = [\"<PAD>\", \"<START>\", \"<END>\", \"<UNK>\"] + [word for word, count in word_counts.items() if count >= 5]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Convert captions to sequences of indices\n",
    "def caption_to_indices(caption):\n",
    "    tokens = [\"<START>\"] + nltk.word_tokenize(caption.lower()) + [\"<END>\"]\n",
    "    return [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in tokens]\n",
    "\n",
    "train_sequences = {path.split(\"/\")[-1]: [caption_to_indices(caption) for caption in captions_dict.get(path.split(\"/\")[-1], [])] for path in train_image_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EncoderCNN\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]  # Remove the last fully connected layer\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)  # Extract features from ResNet\n",
    "        features = features.reshape(features.size(0), -1)  # Flatten features\n",
    "\n",
    "        # Apply BatchNorm only if batch size > 1\n",
    "        if features.size(0) > 1:\n",
    "            features = self.bn(self.linear(features))\n",
    "        else:\n",
    "            features = self.linear(features)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Define the DecoderRNN\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=20):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_output, _ = self.lstm(packed)\n",
    "        outputs = self.linear(lstm_output[0])\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, features, states=None):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for _ in range(self.max_seq_length):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted).unsqueeze(1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0000\n",
      "Epoch [2/10], Loss: 0.0000\n",
      "Epoch [3/10], Loss: 0.0000\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for image_path, caption_seqs in train_sequences.items():\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(os.path.join(images_folder, image_path)).convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    "        features = encoder(image_tensor)\n",
    "\n",
    "        # Train on each caption for the image\n",
    "        for caption_seq in caption_seqs:\n",
    "            caption_tensor = torch.tensor([word_to_idx[\"<START>\"]] + caption_seq + [word_to_idx[\"<END>\"]]).unsqueeze(0).to(device)\n",
    "            lengths = [len(caption_seq) + 2]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = decoder(features, caption_tensor, lengths)\n",
    "            targets = pack_padded_sequence(caption_tensor[:, 1:], lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: <UNK> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START>\n"
     ]
    }
   ],
   "source": [
    "# Generate captions for a test image\n",
    "test_image_path = test_image_paths[0]\n",
    "test_image = Image.open(test_image_path).convert('RGB')\n",
    "test_image_tensor = transforms.ToTensor()(test_image).unsqueeze(0).to(device)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    test_features = encoder(test_image_tensor)\n",
    "    sampled_ids = decoder.predict(test_features)\n",
    "    sampled_caption = [idx_to_word[idx.item()] for idx in sampled_ids[0] if idx.item() != word_to_idx[\"<PAD>\"]]\n",
    "    print(\"Generated Caption:\", \" \".join(sampled_caption))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
